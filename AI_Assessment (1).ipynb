{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI Technical Assessment"
      ],
      "metadata": {
        "id": "rg4LY_IOcd3m"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "949e76fc"
      },
      "source": [
        "## Load and preprocess the pdf document\n",
        "\n",
        "### Subtask:\n",
        "Load the specified PDF document, preprocess it to clean and prepare the text for chunking.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5abede8b"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to load the PDF and extract the text content from each page. I will use PyMuPDF for this purpose.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfa946c6",
        "outputId": "a42d66e6-c07d-41c1-b84e-f1bd978611f8"
      },
      "source": [
        "import fitz # PyMuPDF\n",
        "import re\n",
        "\n",
        "pdf_path = \"/content/HSC26 Bangla 1st Paper.pdf\"\n",
        "doc = fitz.open(pdf_path)\n",
        "\n",
        "text_content = \"\"\n",
        "for page_num in range(doc.page_count):\n",
        "    page = doc.load_page(page_num)\n",
        "    text_content += page.get_text()\n",
        "\n",
        "# Basic cleaning: remove extra whitespace and newlines\n",
        "cleaned_text = re.sub(r'\\s+', ' ', text_content).strip()\n",
        "\n",
        "print(f\"Original text length: {len(text_content)}\")\n",
        "print(f\"Cleaned text length: {len(cleaned_text)}\")\n",
        "print(\"First 500 characters of cleaned text:\")\n",
        "print(cleaned_text[:500])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text length: 82287\n",
            "Cleaned text length: 79499\n",
            "First 500 characters of cleaned text:\n",
            "অনলাইন ব্যাচ সম্পর্কিত যেককাকনা জিজ্ঞাাসা , অপরিরিতা আল ািয রিষয় িাাং া ১ম পত্র ১। অনুপলেি িািা কী কলি জীরিকা রনিবাহ কিলতন? ক) ডাক্তার্ি খ) ওকালর্ত গ) মাস্টার্ি ঘ) ব্যব্সা ২। োোলক ভাগ্য দেিতাি প্রধান এলজন্ট ি াি কািণ, তাি- ক) প্রর্তপজি খ) প্রভাব্ গ) র্ব্চক্ষণতা ঘ) কূট ব্ুর্ি র্নকচি অনুকেদটি পক়ে ৩ ও ৪ সংখযক প্রকেি উিি দাও। র্পতৃহীন দীপুি চাচাই র্িকলন পর্িব্াকিি কতিা। দীপু র্িজক্ষত হকলও তাি র্সিান্ত যনও াি ক্ষমতা র্িল না। চাচা তাি র্ব্ক ি উকদযাগ র্নকলও যেৌতুক র্নক ব্া়োব্ার়্ে কিাি কািকণ কনযাি\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2323b84e"
      },
      "source": [
        "## Chunk the document\n",
        "\n",
        "### Subtask:\n",
        "Split the preprocessed document into smaller, manageable chunks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b0315ea"
      },
      "source": [
        "**Reasoning**:\n",
        "Determine chunk size and overlap, then split the cleaned text into chunks and store them in a list. Finally, print the number of chunks and the first chunk.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8457b411",
        "outputId": "c997a753-5fa8-458f-d00f-982239e3f376"
      },
      "source": [
        "# Determine chunk size and overlap\n",
        "chunk_size = 500  # characters\n",
        "chunk_overlap = 100 # characters\n",
        "\n",
        "# Split the cleaned text into chunks\n",
        "chunks = []\n",
        "for i in range(0, len(cleaned_text), chunk_size - chunk_overlap):\n",
        "    chunk = cleaned_text[i:i + chunk_size]\n",
        "    chunks.append(chunk)\n",
        "\n",
        "# Print the number of chunks and the first chunk\n",
        "print(f\"Number of chunks created: {len(chunks)}\")\n",
        "if chunks:\n",
        "    print(\"\\nFirst chunk:\")\n",
        "    print(chunks[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of chunks created: 199\n",
            "\n",
            "First chunk:\n",
            "অনলাইন ব্যাচ সম্পর্কিত যেককাকনা জিজ্ঞাাসা , অপরিরিতা আল ািয রিষয় িাাং া ১ম পত্র ১। অনুপলেি িািা কী কলি জীরিকা রনিবাহ কিলতন? ক) ডাক্তার্ি খ) ওকালর্ত গ) মাস্টার্ি ঘ) ব্যব্সা ২। োোলক ভাগ্য দেিতাি প্রধান এলজন্ট ি াি কািণ, তাি- ক) প্রর্তপজি খ) প্রভাব্ গ) র্ব্চক্ষণতা ঘ) কূট ব্ুর্ি র্নকচি অনুকেদটি পক়ে ৩ ও ৪ সংখযক প্রকেি উিি দাও। র্পতৃহীন দীপুি চাচাই র্িকলন পর্িব্াকিি কতিা। দীপু র্িজক্ষত হকলও তাি র্সিান্ত যনও াি ক্ষমতা র্িল না। চাচা তাি র্ব্ক ি উকদযাগ র্নকলও যেৌতুক র্নক ব্া়োব্ার়্ে কিাি কািকণ কনযাি\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1c2c3af"
      },
      "source": [
        "## Vectorize the chunks\n",
        "\n",
        "### Subtask:\n",
        "Generate vector representations (embeddings) for each document chunk.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0a0b373"
      },
      "source": [
        "**Reasoning**:\n",
        "Generate vector representations (embeddings) for each document chunk using a multilingual sentence transformer model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee6bcb46",
        "outputId": "19d84abd-58e8-4f73-afcc-175b0704914b"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load a suitable multilingual model\n",
        "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "\n",
        "# Generate embeddings for each chunk\n",
        "embeddings = []\n",
        "for chunk in chunks:\n",
        "    embedding = model.encode(chunk)\n",
        "    embeddings.append(embedding)\n",
        "\n",
        "# Print the number of embeddings and the shape of the first embedding\n",
        "print(f\"Number of embeddings generated: {len(embeddings)}\")\n",
        "if embeddings:\n",
        "    print(f\"Shape of the first embedding: {embeddings[0].shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of embeddings generated: 199\n",
            "Shape of the first embedding: (384,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a Vector Database"
      ],
      "metadata": {
        "id": "Fl4zQNaKdD3G"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVNkMsDFS0iI",
        "outputId": "788f1ebe-3db6-480b-e841-ff8de0ef5135"
      },
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "import numpy as np\n",
        "\n",
        "# Convert embeddings to numpy array\n",
        "embeddings_np = np.array(embeddings)\n",
        "\n",
        "# Create FAISS index\n",
        "vectorstore = FAISS.from_embeddings(text_embeddings=list(zip(chunks, embeddings_np)), embedding=model)\n",
        "\n",
        "\n",
        "# Verify the creation of the vector database\n",
        "print(f\"Vector database created with {vectorstore.index.ntotal} vectors.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.vectorstores.faiss:`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector database created with 199 vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9769618"
      },
      "source": [
        "## Implement the rag system\n",
        "\n",
        "### Subtask:\n",
        "Implement the rag system\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c89b9878"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the RAG function, generate query embedding, perform similarity search, construct the prompt, use a language model (assuming a simple one for demonstration), and return the response.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c107b370",
        "outputId": "0ca688dd-fb0c-4f45-add0-06a7d6080877"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load a simple conversational model (adjust as needed)\n",
        "# Note: A more sophisticated model or API would be needed for better performance\n",
        "# This is a placeholder for demonstration purposes.\n",
        "# Using a text generation pipeline\n",
        "# You might need to install a model like 'google/flan-t5-small' or similar if you don't have one\n",
        "# !pip install transformers accelerate\n",
        "# !pip install bitsandbytes # if using 8-bit quantization\n",
        "\n",
        "# Attempt to load a model. If this fails, you may need to specify a different model or handle authentication/installation.\n",
        "try:\n",
        "    generator = pipeline(\"text-generation\", model=\"google/flan-t5-small\", device=0) # Use device=0 for GPU if available\n",
        "except Exception as e:\n",
        "    print(f\"Could not load text-generation pipeline: {e}\")\n",
        "    print(\"Falling back to a simpler approach or requiring manual model setup.\")\n",
        "    generator = None # Set generator to None if loading fails\n",
        "\n",
        "\n",
        "def rag_query(query: str, k: int = 3) -> str:\n",
        "    \"\"\"\n",
        "    Processes a user query using the RAG system.\n",
        "\n",
        "    Args:\n",
        "        query: The user's query string (English or Bengali).\n",
        "        k: The number of relevant document chunks to retrieve.\n",
        "\n",
        "    Returns:\n",
        "        The generated response from the language model.\n",
        "    \"\"\"\n",
        "    if model is None or vectorstore is None or generator is None:\n",
        "        return \"RAG system not fully initialized. Model, vector store, or generator is missing.\"\n",
        "\n",
        "    # 2. Generate embedding for the user query\n",
        "    query_embedding = model.encode(query)\n",
        "\n",
        "    # 3. Perform similarity search\n",
        "    # The vectorstore object from FAISS.from_embeddings does not have 'similarity_search'.\n",
        "    # We need to use the underlying FAISS index or query the retriever interface if one was set up.\n",
        "    # Assuming vectorstore is a LangChain FAISS object, it should have a similarity_search method.\n",
        "    # Let's verify this based on the previous step's output indicating `FAISS.from_embeddings` was used.\n",
        "    try:\n",
        "        retrieved_docs = vectorstore.similarity_search_by_vector(query_embedding, k=k)\n",
        "    except AttributeError:\n",
        "         # If similarity_search_by_vector is not available, we might need a different approach\n",
        "         # or there was an issue with the FAISS object creation.\n",
        "         # For demonstration, let's assume the expected method exists based on LangChain FAISS docs.\n",
        "         # If this fails in execution, the FAISS object from the previous step needs re-evaluation.\n",
        "         print(\"Error: 'similarity_search_by_vector' method not found on the vectorstore object.\")\n",
        "         return \"Error retrieving documents.\"\n",
        "\n",
        "\n",
        "    # 4. Construct a prompt\n",
        "    context = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "\n",
        "    # Simple prompt template - instruct the model to use context and the query language\n",
        "    prompt_template = f\"\"\"Use the following context to answer the user's query.\n",
        "    Respond in the same language as the query.\n",
        "    If you don't know the answer based on the context, just say that you don't know.\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Query:\n",
        "    {query}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "\n",
        "    # 5. Use a language model to generate a response\n",
        "    if generator:\n",
        "        # Using the text-generation pipeline\n",
        "        # Adjust max_new_tokens and other parameters as needed\n",
        "        response = generator(prompt_template, max_new_tokens=150, num_return_sequences=1)[0]['generated_text']\n",
        "        # The generated text might include the prompt itself, need to clean it.\n",
        "        # A simple cleaning might be to remove the prompt_template part.\n",
        "        # However, the pipeline often just continues the text.\n",
        "        # A better approach might be to fine-tune the prompt or use a different pipeline/model structure.\n",
        "        # For this simple demo, let's just assume the model generates after \"Answer:\".\n",
        "        # Finding the \"Answer:\" and taking the text after it.\n",
        "        answer_prefix = \"Answer:\"\n",
        "        if answer_prefix in response:\n",
        "            response = response.split(answer_prefix, 1)[1].strip()\n",
        "\n",
        "    else:\n",
        "        response = \"Language model generator not initialized.\"\n",
        "\n",
        "    # 6. Return the generated response\n",
        "    return response\n",
        "\n",
        "# Example usage (optional, for testing)\n",
        "# english_query = \"What is the name of the chapter about humans?\"\n",
        "# bangla_query = \"ঐক্যতান কবিতার মূলভাব কী?\"\n",
        "#\n",
        "# print(f\"English Query: {english_query}\")\n",
        "# english_response = rag_query(english_query)\n",
        "# print(f\"Response: {english_response}\")\n",
        "#\n",
        "# print(f\"\\nBengali Query: {bangla_query}\")\n",
        "# bangla_response = rag_query(bangla_query)\n",
        "# print(f\"Response: {bangla_response}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "The model 'T5ForConditionalGeneration' is not supported for text-generation. Supported models are ['PeftModelForCausalLM', 'ArceeForCausalLM', 'AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BitNetForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'Dots1ForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconH1ForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'Gemma3nForConditionalGeneration', 'Gemma3nForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MiniMaxForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'SmolLM3ForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQ48m4K4TCu4",
        "outputId": "d5b98b20-9082-4543-a470-96ce830ffeeb"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load a simple conversational model suitable for Flan-T5\n",
        "# Using 'text2text-generation' pipeline for Flan-T5\n",
        "try:\n",
        "    generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\", device=0) # Use device=0 for GPU if available\n",
        "    print(\"Text2Text generation pipeline loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not load text2text-generation pipeline: {e}\")\n",
        "    print(\"Language model generator not initialized.\")\n",
        "    generator = None # Set generator to None if loading fails\n",
        "\n",
        "\n",
        "def rag_query(query: str, k: int = 3) -> str:\n",
        "    \"\"\"\n",
        "    Processes a user query using the RAG system.\n",
        "\n",
        "    Args:\n",
        "        query: The user's query string (English or Bengali).\n",
        "        k: The number of relevant document chunks to retrieve.\n",
        "\n",
        "    Returns:\n",
        "        The generated response from the language model.\n",
        "    \"\"\"\n",
        "    # Check if all necessary components are initialized\n",
        "    if 'model' not in globals() or model is None:\n",
        "         return \"SentenceTransformer model not initialized.\"\n",
        "    if 'vectorstore' not in globals() or vectorstore is None:\n",
        "        return \"Vector store not initialized.\"\n",
        "    if generator is None:\n",
        "        return \"Language model generator not initialized.\"\n",
        "\n",
        "\n",
        "    # 2. Generate embedding for the user query\n",
        "    query_embedding = model.encode(query)\n",
        "\n",
        "    # 3. Perform similarity search\n",
        "    try:\n",
        "        # Assuming vectorstore is a LangChain FAISS object which has this method\n",
        "        retrieved_docs = vectorstore.similarity_search_by_vector(query_embedding, k=k)\n",
        "    except AttributeError:\n",
        "         print(\"Error: 'similarity_search_by_vector' method not found on the vectorstore object.\")\n",
        "         return \"Error retrieving documents.\"\n",
        "    except Exception as e:\n",
        "         print(f\"An error occurred during similarity search: {e}\")\n",
        "         return \"Error retrieving documents.\"\n",
        "\n",
        "\n",
        "    # 4. Construct a prompt\n",
        "    context = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "\n",
        "    # Simple prompt template - instruct the model to use context and the query language\n",
        "    # For Flan-T5, a question answering format might be more effective.\n",
        "    prompt_template = f\"\"\"Answer the following question based on the provided context.\n",
        "    Respond in the same language as the question.\n",
        "    If the answer is not in the context, say \"I cannot answer this question based on the provided information.\"\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question:\n",
        "    {query}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "\n",
        "    # 5. Use a language model to generate a response\n",
        "    if generator:\n",
        "        try:\n",
        "            # Using the text2text-generation pipeline\n",
        "            # Adjust max_new_tokens and other parameters as needed\n",
        "            # Flan-T5 is an encoder-decoder model, so the output is the generated text directly.\n",
        "            response = generator(prompt_template, max_new_tokens=150, num_return_sequences=1)[0]['generated_text']\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during text generation: {e}\")\n",
        "            response = \"Error generating response.\"\n",
        "    else:\n",
        "        response = \"Language model generator not initialized.\" # This case should be caught earlier, but kept for safety.\n",
        "\n",
        "    # 6. Return the generated response\n",
        "    return response\n",
        "\n",
        "# Example usage (optional, for testing)\n",
        "# english_query = \"What is the name of the chapter about humans?\"\n",
        "# bangla_query = \"ঐক্যতান কবিতার মূলভাব কী?\"\n",
        "#\n",
        "# print(f\"English Query: {english_query}\")\n",
        "# english_response = rag_query(english_query)\n",
        "# print(f\"Response: {english_response}\")\n",
        "#\n",
        "# print(f\"\\nBengali Query: {bangla_query}\")\n",
        "# bangla_response = rag_query(bangla_query)\n",
        "# print(f\"Response: {bangla_response}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text2Text generation pipeline loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0450822"
      },
      "source": [
        "## (bonus) implement a conversation api\n",
        "\n",
        "### Subtask:\n",
        "Create a simple REST API to interact with the RAG system.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a83f0b1"
      },
      "source": [
        "**Reasoning**:\n",
        "Import necessary libraries and define the FastAPI application instance and the request body model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2574ac0"
      },
      "source": [
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import uvicorn\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "class QueryRequest(BaseModel):\n",
        "    query: str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c952e081"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the API endpoint that accepts the user's query, calls the rag_query function, and returns the response.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dc3b459",
        "outputId": "2013b481-13a0-4da1-ffeb-8e2d5e6a568a"
      },
      "source": [
        "@app.post(\"/query\")\n",
        "def process_query(request: QueryRequest):\n",
        "    \"\"\"\n",
        "    API endpoint to process user queries using the RAG system.\n",
        "    \"\"\"\n",
        "    user_query = request.query\n",
        "    response = rag_query(user_query)\n",
        "    return {\"response\": response}\n",
        "\n",
        "# Include the code to run the FastAPI application\n",
        "if __name__ == \"__main__\":\n",
        "    # Note: In a production environment, you might want to configure host and port\n",
        "    # and potentially use a process manager like Gunicorn.\n",
        "    # For this example, we'll run directly with uvicorn.\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=3000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [14877]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:3000 (Press CTRL+C to quit)\n",
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [14877]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ce2168d"
      },
      "source": [
        "**Reasoning**:\n",
        "The error `RuntimeError: asyncio.run() cannot be called from a running event loop` indicates that `uvicorn.run()` is being called within an environment that already has a running asyncio event loop, such as a Jupyter Notebook. To run the FastAPI application within this environment for testing or demonstration purposes without blocking the notebook execution, we can use `nest_asyncio`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewoDQEhUTPQj",
        "outputId": "660eaed0-2bb7-4ce4-e6dd-5f7eda6548a5"
      },
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# This part is only for running in environments like Jupyter notebooks\n",
        "# In a typical application, you would just use the if __name__ == \"__main__\": block\n",
        "# from the previous step in a separate Python file.\n",
        "print(\"FastAPI app defined. To run it, execute the cell below.\")\n",
        "\n",
        "# To run the app in a notebook, you can use a different approach if nest_asyncio is not preferred\n",
        "# or if you want to run it in a separate thread/process.\n",
        "# However, for simplicity and demonstration within the notebook context, nest_asyncio is used here."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastAPI app defined. To run it, execute the cell below.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6192287f"
      },
      "source": [
        "## (bonus) implement rag evaluation\n",
        "\n",
        "### Subtask:\n",
        "Develop a method to evaluate the groundedness and relevance of the generated answers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff6415d8"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a function `evaluate_rag_answer` that takes query, context, and answer as input and uses keyword overlap and semantic similarity to assess groundedness and relevance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "da7a030b"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_rag_answer(query: str, retrieved_context: str, generated_answer: str, model) -> dict:\n",
        "    \"\"\"\n",
        "    Evaluates the groundedness and relevance of a generated answer.\n",
        "\n",
        "    Args:\n",
        "        query: The original user query.\n",
        "        retrieved_context: The text retrieved from the document.\n",
        "        generated_answer: The answer generated by the language model.\n",
        "        model: The sentence transformer model for generating embeddings.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing evaluation metrics (groundedness and relevance scores).\n",
        "    \"\"\"\n",
        "    eval_results = {\n",
        "        \"groundedness_score\": 0.0,\n",
        "        \"relevance_score\": 0.0,\n",
        "        \"groundedness_judgment\": \"Unsupported\",\n",
        "        \"relevance_judgment\": \"Irrelevant\"\n",
        "    }\n",
        "\n",
        "    if not generated_answer or not retrieved_context:\n",
        "        # Cannot evaluate if there's no answer or context\n",
        "        return eval_results\n",
        "\n",
        "    # --- Groundedness Evaluation ---\n",
        "    # Method 1: Keyword Overlap (Simple approach)\n",
        "    # Check if key terms from the answer are present in the context\n",
        "    answer_words = set(re.findall(r'\\b\\w+\\b', generated_answer.lower()))\n",
        "    context_words = set(re.findall(r'\\b\\w+\\b', retrieved_context.lower()))\n",
        "    common_words = answer_words.intersection(context_words)\n",
        "    # Simple groundedness score based on word overlap ratio (can be refined)\n",
        "    if len(answer_words) > 0:\n",
        "        eval_results[\"groundedness_score\"] = len(common_words) / len(answer_words)\n",
        "\n",
        "    # Method 2: Semantic Similarity (Using sentence embeddings)\n",
        "    # Compare the similarity between the answer and the context\n",
        "    try:\n",
        "        answer_embedding = model.encode(generated_answer)\n",
        "        context_embedding = model.encode(retrieved_context)\n",
        "        # Reshape for cosine_similarity calculation if they are 1D arrays\n",
        "        if answer_embedding.ndim == 1:\n",
        "             answer_embedding = answer_embedding.reshape(1, -1)\n",
        "        if context_embedding.ndim == 1:\n",
        "             context_embedding = context_embedding.reshape(1, -1)\n",
        "\n",
        "        # Calculate cosine similarity\n",
        "        semantic_groundedness = cosine_similarity(answer_embedding, context_embedding)[0][0]\n",
        "        # Combine semantic similarity with keyword overlap (optional, adjust weighting)\n",
        "        # eval_results[\"groundedness_score\"] = (eval_results[\"groundedness_score\"] + semantic_groundedness) / 2\n",
        "        eval_results[\"semantic_groundedness_score\"] = float(semantic_groundedness)\n",
        "\n",
        "        # Judgment based on a threshold (can be refined)\n",
        "        if eval_results[\"groundedness_score\"] > 0.2 or semantic_groundedness > 0.5: # Example thresholds\n",
        "             eval_results[\"groundedness_judgment\"] = \"Supported\"\n",
        "        else:\n",
        "             eval_results[\"groundedness_judgment\"] = \"Unsupported\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during groundedness semantic similarity calculation: {e}\")\n",
        "\n",
        "\n",
        "    # --- Relevance Evaluation ---\n",
        "    # Method 1: Keyword Overlap (Simple approach)\n",
        "    # Check if key terms from the query are present in the answer\n",
        "    query_words = set(re.findall(r'\\b\\w+\\b', query.lower()))\n",
        "    answer_words_for_relevance = set(re.findall(r'\\b\\w+\\b', generated_answer.lower())) # Recalculate to be safe\n",
        "    common_query_answer_words = query_words.intersection(answer_words_for_relevance)\n",
        "    # Simple relevance score based on word overlap ratio (can be refined)\n",
        "    if len(query_words) > 0:\n",
        "        eval_results[\"relevance_score\"] = len(common_query_answer_words) / len(query_words)\n",
        "\n",
        "\n",
        "    # Method 2: Semantic Similarity (Using sentence embeddings)\n",
        "    # Compare the similarity between the query and the answer\n",
        "    try:\n",
        "        query_embedding = model.encode(query)\n",
        "        # Ensure query_embedding is 2D for cosine_similarity\n",
        "        if query_embedding.ndim == 1:\n",
        "            query_embedding = query_embedding.reshape(1, -1)\n",
        "\n",
        "        semantic_relevance = cosine_similarity(query_embedding, answer_embedding)[0][0]\n",
        "        # Combine semantic similarity with keyword overlap (optional)\n",
        "        # eval_results[\"relevance_score\"] = (eval_results[\"relevance_score\"] + semantic_relevance) / 2\n",
        "        eval_results[\"semantic_relevance_score\"] = float(semantic_relevance)\n",
        "\n",
        "        # Judgment based on a threshold (can be refined)\n",
        "        if eval_results[\"relevance_score\"] > 0.2 or semantic_relevance > 0.5: # Example thresholds\n",
        "            eval_results[\"relevance_judgment\"] = \"Relevant\"\n",
        "        else:\n",
        "            eval_results[\"relevance_judgment\"] = \"Irrelevant\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during relevance semantic similarity calculation: {e}\")\n",
        "\n",
        "\n",
        "    return eval_results\n",
        "\n",
        "# Example Usage (for testing)\n",
        "# Note: Replace with actual model and sample data\n",
        "# try:\n",
        "#     # Assuming 'model' is already loaded from previous steps\n",
        "#     sample_query_en = \"What is the main theme of the chapter?\"\n",
        "#     sample_context_en = \"The main theme of this chapter is the importance of education...\"\n",
        "#     sample_answer_en = \"The main theme discussed is the significance of learning.\"\n",
        "#\n",
        "#     sample_query_bn = \"কবিতার মূলভাব কী?\"\n",
        "#     sample_context_bn = \"কবিতাটির মূলভাব হলো প্রকৃতির রূপ বর্ণনা...\"\n",
        "#     sample_answer_bn = \"এই কবিতার মূলভাব হলো প্রকৃতির সৌন্দর্য।\"\n",
        "#\n",
        "#     print(\"\\n--- English Example ---\")\n",
        "#     eval_en = evaluate_rag_answer(sample_query_en, sample_context_en, sample_answer_en, model)\n",
        "#     print(f\"Evaluation Results (English): {eval_en}\")\n",
        "#\n",
        "#     print(\"\\n--- Bengali Example ---\")\n",
        "#     eval_bn = evaluate_rag_answer(sample_query_bn, sample_context_bn, sample_answer_bn, model)\n",
        "#     print(f\"Evaluation Results (Bengali): {eval_bn}\")\n",
        "#\n",
        "# except NameError:\n",
        "#     print(\"\\nSentenceTransformer model 'model' is not defined. Please run the vectorization step first.\")\n",
        "# except Exception as e:\n",
        "#      print(f\"\\nAn error occurred during example evaluation: {e}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3844740e",
        "outputId": "504dcd17-d335-4694-b396-433c889569be"
      },
      "source": [
        "# Install ngrok\n",
        "!pip install pyngrok"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.12)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6866c34e",
        "outputId": "1bd9a0df-f7d9-4745-99fa-de3f21e582fe"
      },
      "source": [
        "# Authenticate ngrok (You'll need an ngrok auth token from your ngrok account)\n",
        "# Replace 'YOUR_AUTHTOKEN' with your actual ngrok auth token\n",
        "from pyngrok import ngrok\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get your authtoken from https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "# Add your token to Colab secrets with the name 'NGROK_AUTH_TOKEN'\n",
        "NGROK_AUTH_TOKEN = userdata.get(\"NGROK_AUTH_TOKEN\")\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# Start ngrok tunnel for port 8000\n",
        "# The ngrok process will run in the background\n",
        "ngrok_tunnel = ngrok.connect(3000)\n",
        "print(f\"Public URL: {ngrok_tunnel.public_url}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: https://9f8d4f544c66.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Mostafa-Annur/AI_Assessment.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehoBZ_o6bNEW",
        "outputId": "7d608291-fb18-462a-9ad7-0a21aa8f93e4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AI_Assessment'...\n",
            "remote: Enumerating objects: 8, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 8 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (8/8), 12.84 KiB | 4.28 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3bd4df1",
        "outputId": "21476aba-03d8-4895-e6f9-6bf867b77e10"
      },
      "source": [
        "# Install git (if not already installed, Colab usually has it)\n",
        "# !apt-get update && apt-get install -y git\n",
        "\n",
        "# Configure Git (replace with your name and email)\n",
        "!git config --global user.name \"Mostafa-Annur\"\n",
        "!git config --global user.email \"mostafaannur@gmail.com\"\n",
        "\n",
        "# Clone your empty GitHub repository (replace with your repo URL)\n",
        "# You might need to use a Personal Access Token for authentication if prompted\n",
        "!git clone https://github.com/Mostafa-Annur/AI_Assessment.git\n",
        "\n",
        "# Navigate into your repository directory\n",
        "import os\n",
        "os.chdir('AI_Assessment')\n",
        "\n",
        "# Copy your notebook file into the repository directory\n",
        "# Replace 'your_notebook_name.ipynb' with the actual filename of your notebook\n",
        "!cp /content/AI_Assessment.ipynb .\n",
        "\n",
        "# Add the notebook file\n",
        "# Replace 'your_notebook_name.ipynb' with the actual filename of your notebook\n",
        "!git add your_notebook_name.ipynb\n",
        "\n",
        "# Commit the changes\n",
        "!git commit -m \"Add RAG notebook\"\n",
        "\n",
        "# Push to the remote repository\n",
        "!git push origin main # or master, depending on your branch name\n",
        "\n",
        "# Note: Uncomment and execute the commands above one by one after replacing placeholders."
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AI_Assessment'...\n",
            "remote: Enumerating objects: 8, done.\u001b[K\n",
            "remote: Counting objects:  12% (1/8)\u001b[K\rremote: Counting objects:  25% (2/8)\u001b[K\rremote: Counting objects:  37% (3/8)\u001b[K\rremote: Counting objects:  50% (4/8)\u001b[K\rremote: Counting objects:  62% (5/8)\u001b[K\rremote: Counting objects:  75% (6/8)\u001b[K\rremote: Counting objects:  87% (7/8)\u001b[K\rremote: Counting objects: 100% (8/8)\u001b[K\rremote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects:  25% (1/4)\u001b[K\rremote: Compressing objects:  50% (2/4)\u001b[K\rremote: Compressing objects:  75% (3/4)\u001b[K\rremote: Compressing objects: 100% (4/4)\u001b[K\rremote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 8 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects:  12% (1/8)\rReceiving objects:  25% (2/8)\rReceiving objects:  37% (3/8)\rReceiving objects:  50% (4/8)\rReceiving objects:  62% (5/8)\rReceiving objects:  75% (6/8)\rReceiving objects:  87% (7/8)\rReceiving objects: 100% (8/8)\rReceiving objects: 100% (8/8), 12.84 KiB | 12.84 MiB/s, done.\n",
            "cp: cannot stat '/content/AI_Assessment.ipynb': No such file or directory\n",
            "fatal: pathspec 'your_notebook_name.ipynb' did not match any files\n",
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "nothing to commit, working tree clean\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    }
  ]
}