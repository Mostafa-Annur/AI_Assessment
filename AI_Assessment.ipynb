{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI Technical Assessment"
      ],
      "metadata": {
        "id": "rg4LY_IOcd3m"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "949e76fc"
      },
      "source": [
        "## Load and preprocess the pdf document\n",
        "\n",
        "### Subtask:\n",
        "Load the specified PDF document, preprocess it to clean and prepare the text for chunking.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd23d32f",
        "outputId": "4e900264-fc54-49ea-aa83-c3c482804de6"
      },
      "source": [
        "!pip install PyMuPDF"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (1.26.3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.11/pathlib.py:540: RuntimeWarning: coroutine 'Server.serve' was never awaited\n",
            "  return self._str\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5abede8b"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to load the PDF and extract the text content from each page. I will use PyMuPDF for this purpose.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfa946c6",
        "outputId": "2a7dc5d3-cd41-4715-fac0-e94b0fd8592d"
      },
      "source": [
        "import fitz # PyMuPDF\n",
        "import re\n",
        "\n",
        "pdf_path = \"/content/HSC26-Bangla1st-Paper.pdf\"\n",
        "doc = fitz.open(pdf_path)\n",
        "\n",
        "text_content = \"\"\n",
        "for page_num in range(doc.page_count):\n",
        "    page = doc.load_page(page_num)\n",
        "    text_content += page.get_text()\n",
        "\n",
        "# Basic cleaning: remove extra whitespace and newlines\n",
        "cleaned_text = re.sub(r'\\s+', ' ', text_content).strip()\n",
        "\n",
        "print(f\"Original text length: {len(text_content)}\")\n",
        "print(f\"Cleaned text length: {len(cleaned_text)}\")\n",
        "print(\"First 500 characters of cleaned text:\")\n",
        "print(cleaned_text[:500])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text length: 82287\n",
            "Cleaned text length: 79499\n",
            "First 500 characters of cleaned text:\n",
            "অনলাইন ব্যাচ সম্পর্কিত যেককাকনা জিজ্ঞাাসা , অপরিরিতা আল ািয রিষয় িাাং া ১ম পত্র ১। অনুপলেি িািা কী কলি জীরিকা রনিবাহ কিলতন? ক) ডাক্তার্ি খ) ওকালর্ত গ) মাস্টার্ি ঘ) ব্যব্সা ২। োোলক ভাগ্য দেিতাি প্রধান এলজন্ট ি াি কািণ, তাি- ক) প্রর্তপজি খ) প্রভাব্ গ) র্ব্চক্ষণতা ঘ) কূট ব্ুর্ি র্নকচি অনুকেদটি পক়ে ৩ ও ৪ সংখযক প্রকেি উিি দাও। র্পতৃহীন দীপুি চাচাই র্িকলন পর্িব্াকিি কতিা। দীপু র্িজক্ষত হকলও তাি র্সিান্ত যনও াি ক্ষমতা র্িল না। চাচা তাি র্ব্ক ি উকদযাগ র্নকলও যেৌতুক র্নক ব্া়োব্ার়্ে কিাি কািকণ কনযাি\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2323b84e"
      },
      "source": [
        "## Chunk the document\n",
        "\n",
        "### Subtask:\n",
        "Split the preprocessed document into smaller, manageable chunks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b0315ea"
      },
      "source": [
        "**Reasoning**:\n",
        "Determine chunk size and overlap, then split the cleaned text into chunks and store them in a list. Finally, print the number of chunks and the first chunk.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8457b411",
        "outputId": "752da778-154e-4c91-d52d-eb0386644117"
      },
      "source": [
        "# Determine chunk size and overlap\n",
        "chunk_size = 500  # characters\n",
        "chunk_overlap = 100 # characters\n",
        "\n",
        "# Split the cleaned text into chunks\n",
        "chunks = []\n",
        "for i in range(0, len(cleaned_text), chunk_size - chunk_overlap):\n",
        "    chunk = cleaned_text[i:i + chunk_size]\n",
        "    chunks.append(chunk)\n",
        "\n",
        "# Print the number of chunks and the first chunk\n",
        "print(f\"Number of chunks created: {len(chunks)}\")\n",
        "if chunks:\n",
        "    print(\"\\nFirst chunk:\")\n",
        "    print(chunks[0])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of chunks created: 199\n",
            "\n",
            "First chunk:\n",
            "অনলাইন ব্যাচ সম্পর্কিত যেককাকনা জিজ্ঞাাসা , অপরিরিতা আল ািয রিষয় িাাং া ১ম পত্র ১। অনুপলেি িািা কী কলি জীরিকা রনিবাহ কিলতন? ক) ডাক্তার্ি খ) ওকালর্ত গ) মাস্টার্ি ঘ) ব্যব্সা ২। োোলক ভাগ্য দেিতাি প্রধান এলজন্ট ি াি কািণ, তাি- ক) প্রর্তপজি খ) প্রভাব্ গ) র্ব্চক্ষণতা ঘ) কূট ব্ুর্ি র্নকচি অনুকেদটি পক়ে ৩ ও ৪ সংখযক প্রকেি উিি দাও। র্পতৃহীন দীপুি চাচাই র্িকলন পর্িব্াকিি কতিা। দীপু র্িজক্ষত হকলও তাি র্সিান্ত যনও াি ক্ষমতা র্িল না। চাচা তাি র্ব্ক ি উকদযাগ র্নকলও যেৌতুক র্নক ব্া়োব্ার়্ে কিাি কািকণ কনযাি\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1c2c3af"
      },
      "source": [
        "## Vectorize the chunks\n",
        "\n",
        "### Subtask:\n",
        "Generate vector representations (embeddings) for each document chunk.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0a0b373"
      },
      "source": [
        "**Reasoning**:\n",
        "Generate vector representations (embeddings) for each document chunk using a multilingual sentence transformer model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee6bcb46",
        "outputId": "3a454c1a-a364-447e-b369-6284814624d1"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load a suitable multilingual model\n",
        "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "\n",
        "# Generate embeddings for each chunk\n",
        "embeddings = []\n",
        "for chunk in chunks:\n",
        "    embedding = model.encode(chunk)\n",
        "    embeddings.append(embedding)\n",
        "\n",
        "# Print the number of embeddings and the shape of the first embedding\n",
        "print(f\"Number of embeddings generated: {len(embeddings)}\")\n",
        "if embeddings:\n",
        "    print(f\"Shape of the first embedding: {embeddings[0].shape}\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of embeddings generated: 199\n",
            "Shape of the first embedding: (384,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a Vector Database"
      ],
      "metadata": {
        "id": "Fl4zQNaKdD3G"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6837bad2",
        "outputId": "017cf685-8df4-486a-b637-68fa0763bb0a"
      },
      "source": [
        "!pip install langchain-community faiss-cpu"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0.post1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.71)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.26)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.12.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.8)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (4.14.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (3.11.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (2.33.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVNkMsDFS0iI",
        "outputId": "be95d501-c64c-425e-8b38-d7d763f950bf"
      },
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "import numpy as np\n",
        "\n",
        "# Convert embeddings to numpy array\n",
        "embeddings_np = np.array(embeddings)\n",
        "\n",
        "# Create FAISS index\n",
        "vectorstore = FAISS.from_embeddings(text_embeddings=list(zip(chunks, embeddings_np)), embedding=model)\n",
        "\n",
        "\n",
        "# Verify the creation of the vector database\n",
        "print(f\"Vector database created with {vectorstore.index.ntotal} vectors.\")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.vectorstores.faiss:`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector database created with 199 vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9769618"
      },
      "source": [
        "## Implement the rag system\n",
        "\n",
        "### Subtask:\n",
        "Implement the rag system\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c89b9878"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the RAG function, generate query embedding, perform similarity search, construct the prompt, use a language model (assuming a simple one for demonstration), and return the response.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c107b370",
        "outputId": "383bbbb8-4b33-4f44-e363-4d275bb6402b"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load a simple conversational model (adjust as needed)\n",
        "# Note: A more sophisticated model or API would be needed for better performance\n",
        "# This is a placeholder for demonstration purposes.\n",
        "# Using a text generation pipeline\n",
        "# You might need to install a model like 'google/flan-t5-small' or similar if you don't have one\n",
        "# !pip install transformers accelerate\n",
        "# !pip install bitsandbytes # if using 8-bit quantization\n",
        "\n",
        "# Attempt to load a model. If this fails, you may need to specify a different model or handle authentication/installation.\n",
        "try:\n",
        "    generator = pipeline(\"text-generation\", model=\"google/flan-t5-small\", device=0) # Use device=0 for GPU if available\n",
        "except Exception as e:\n",
        "    print(f\"Could not load text-generation pipeline: {e}\")\n",
        "    print(\"Falling back to a simpler approach or requiring manual model setup.\")\n",
        "    generator = None # Set generator to None if loading fails\n",
        "\n",
        "\n",
        "def rag_query(query: str, k: int = 3) -> str:\n",
        "    \"\"\"\n",
        "    Processes a user query using the RAG system.\n",
        "\n",
        "    Args:\n",
        "        query: The user's query string (English or Bengali).\n",
        "        k: The number of relevant document chunks to retrieve.\n",
        "\n",
        "    Returns:\n",
        "        The generated response from the language model.\n",
        "    \"\"\"\n",
        "    if model is None or vectorstore is None or generator is None:\n",
        "        return \"RAG system not fully initialized. Model, vector store, or generator is missing.\"\n",
        "\n",
        "    # 2. Generate embedding for the user query\n",
        "    query_embedding = model.encode(query)\n",
        "\n",
        "    # 3. Perform similarity search\n",
        "    # The vectorstore object from FAISS.from_embeddings does not have 'similarity_search'.\n",
        "    # We need to use the underlying FAISS index or query the retriever interface if one was set up.\n",
        "    # Assuming vectorstore is a LangChain FAISS object, it should have a similarity_search method.\n",
        "    # Let's verify this based on the previous step's output indicating `FAISS.from_embeddings` was used.\n",
        "    try:\n",
        "        retrieved_docs = vectorstore.similarity_search_by_vector(query_embedding, k=k)\n",
        "    except AttributeError:\n",
        "         # If similarity_search_by_vector is not available, we might need a different approach\n",
        "         # or there was an issue with the FAISS object creation.\n",
        "         # For demonstration, let's assume the expected method exists based on LangChain FAISS docs.\n",
        "         # If this fails in execution, the FAISS object from the previous step needs re-evaluation.\n",
        "         print(\"Error: 'similarity_search_by_vector' method not found on the vectorstore object.\")\n",
        "         return \"Error retrieving documents.\"\n",
        "\n",
        "\n",
        "    # 4. Construct a prompt\n",
        "    context = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "\n",
        "    # Simple prompt template - instruct the model to use context and the query language\n",
        "    prompt_template = f\"\"\"Use the following context to answer the user's query.\n",
        "    Respond in the same language as the query.\n",
        "    If you don't know the answer based on the context, just say that you don't know.\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Query:\n",
        "    {query}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "\n",
        "    # 5. Use a language model to generate a response\n",
        "    if generator:\n",
        "        # Using the text-generation pipeline\n",
        "        # Adjust max_new_tokens and other parameters as needed\n",
        "        response = generator(prompt_template, max_new_tokens=150, num_return_sequences=1)[0]['generated_text']\n",
        "        # The generated text might include the prompt itself, need to clean it.\n",
        "        # A simple cleaning might be to remove the prompt_template part.\n",
        "        # However, the pipeline often just continues the text.\n",
        "        # A better approach might be to fine-tune the prompt or use a different pipeline/model structure.\n",
        "        # For this simple demo, let's just assume the model generates after \"Answer:\".\n",
        "        # Finding the \"Answer:\" and taking the text after it.\n",
        "        answer_prefix = \"Answer:\"\n",
        "        if answer_prefix in response:\n",
        "            response = response.split(answer_prefix, 1)[1].strip()\n",
        "\n",
        "    else:\n",
        "        response = \"Language model generator not initialized.\"\n",
        "\n",
        "    # 6. Return the generated response\n",
        "    return response\n",
        "\n",
        "# Example usage (optional, for testing)\n",
        "# english_query = \"What is the name of the chapter about humans?\"\n",
        "# bangla_query = \"ঐক্যতান কবিতার মূলভাব কী?\"\n",
        "#\n",
        "# print(f\"English Query: {english_query}\")\n",
        "# english_response = rag_query(english_query)\n",
        "# print(f\"Response: {english_response}\")\n",
        "#\n",
        "# print(f\"\\nBengali Query: {bangla_query}\")\n",
        "# bangla_response = rag_query(bangla_query)\n",
        "# print(f\"Response: {bangla_response}\")\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "The model 'T5ForConditionalGeneration' is not supported for text-generation. Supported models are ['PeftModelForCausalLM', 'ArceeForCausalLM', 'AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BitNetForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'Dots1ForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconH1ForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'Gemma3nForConditionalGeneration', 'Gemma3nForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MiniMaxForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'SmolLM3ForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQ48m4K4TCu4",
        "outputId": "65bdfd98-c12d-42e0-dfd4-f5fa2f6e351f"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load a simple conversational model suitable for Flan-T5\n",
        "# Using 'text2text-generation' pipeline for Flan-T5\n",
        "try:\n",
        "    generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\", device=0) # Use device=0 for GPU if available\n",
        "    print(\"Text2Text generation pipeline loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not load text2text-generation pipeline: {e}\")\n",
        "    print(\"Language model generator not initialized.\")\n",
        "    generator = None # Set generator to None if loading fails\n",
        "\n",
        "\n",
        "def rag_query(query: str, k: int = 3) -> str:\n",
        "    \"\"\"\n",
        "    Processes a user query using the RAG system.\n",
        "\n",
        "    Args:\n",
        "        query: The user's query string (English or Bengali).\n",
        "        k: The number of relevant document chunks to retrieve.\n",
        "\n",
        "    Returns:\n",
        "        The generated response from the language model.\n",
        "    \"\"\"\n",
        "    # Check if all necessary components are initialized\n",
        "    if 'model' not in globals() or model is None:\n",
        "         return \"SentenceTransformer model not initialized.\"\n",
        "    if 'vectorstore' not in globals() or vectorstore is None:\n",
        "        return \"Vector store not initialized.\"\n",
        "    if generator is None:\n",
        "        return \"Language model generator not initialized.\"\n",
        "\n",
        "\n",
        "    # 2. Generate embedding for the user query\n",
        "    query_embedding = model.encode(query)\n",
        "\n",
        "    # 3. Perform similarity search\n",
        "    try:\n",
        "        # Assuming vectorstore is a LangChain FAISS object which has this method\n",
        "        retrieved_docs = vectorstore.similarity_search_by_vector(query_embedding, k=k)\n",
        "    except AttributeError:\n",
        "         print(\"Error: 'similarity_search_by_vector' method not found on the vectorstore object.\")\n",
        "         return \"Error retrieving documents.\"\n",
        "    except Exception as e:\n",
        "         print(f\"An error occurred during similarity search: {e}\")\n",
        "         return \"Error retrieving documents.\"\n",
        "\n",
        "\n",
        "    # 4. Construct a prompt\n",
        "    context = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "\n",
        "    # Simple prompt template - instruct the model to use context and the query language\n",
        "    # For Flan-T5, a question answering format might be more effective.\n",
        "    prompt_template = f\"\"\"Answer the following question based on the provided context.\n",
        "    Respond in the same language as the question.\n",
        "    If the answer is not in the context, say \"I cannot answer this question based on the provided information.\"\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question:\n",
        "    {query}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "\n",
        "    # 5. Use a language model to generate a response\n",
        "    if generator:\n",
        "        try:\n",
        "            # Using the text2text-generation pipeline\n",
        "            # Adjust max_new_tokens and other parameters as needed\n",
        "            # Flan-T5 is an encoder-decoder model, so the output is the generated text directly.\n",
        "            response = generator(prompt_template, max_new_tokens=150, num_return_sequences=1)[0]['generated_text']\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during text generation: {e}\")\n",
        "            response = \"Error generating response.\"\n",
        "    else:\n",
        "        response = \"Language model generator not initialized.\" # This case should be caught earlier, but kept for safety.\n",
        "\n",
        "    # 6. Return the generated response\n",
        "    return response\n",
        "\n",
        "# Example usage (optional, for testing)\n",
        "# english_query = \"What is the name of the chapter about humans?\"\n",
        "# bangla_query = \"ঐক্যতান কবিতার মূলভাব কী?\"\n",
        "#\n",
        "# print(f\"English Query: {english_query}\")\n",
        "# english_response = rag_query(english_query)\n",
        "# print(f\"Response: {english_response}\")\n",
        "#\n",
        "# print(f\"\\nBengali Query: {bangla_query}\")\n",
        "# bangla_response = rag_query(bangla_query)\n",
        "# print(f\"Response: {bangla_response}\")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text2Text generation pipeline loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0450822"
      },
      "source": [
        "## (bonus) implement a conversation api\n",
        "\n",
        "### Subtask:\n",
        "Create a simple REST API to interact with the RAG system.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a83f0b1"
      },
      "source": [
        "**Reasoning**:\n",
        "Import necessary libraries and define the FastAPI application instance and the request body model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2574ac0"
      },
      "source": [
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import uvicorn\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "class QueryRequest(BaseModel):\n",
        "    query: str"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c952e081"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the API endpoint that accepts the user's query, calls the rag_query function, and returns the response.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6237248",
        "outputId": "fc7f5955-a652-42ba-c8e3-0993cd6e3b7f"
      },
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# This part is only for running in environments like Jupyter notebooks\n",
        "# In a typical application, you would just use the if __name__ == \"__main__\": block\n",
        "# from the previous step in a separate Python file.\n",
        "print(\"FastAPI app defined. To run it, execute the cell below.\")\n",
        "\n",
        "# To run the app in a notebook, you can use a different approach if nest_asyncio is not preferred\n",
        "# or if you want to run it in a separate thread/process.\n",
        "# However, for simplicity and demonstration within the notebook context, nest_asyncio is used here."
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastAPI app defined. To run it, execute the cell below.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dc3b459",
        "outputId": "7ca78808-902b-419b-83e6-1a007b6f1e1f"
      },
      "source": [
        "@app.post(\"/query\")\n",
        "def process_query(request: QueryRequest):\n",
        "    \"\"\"\n",
        "    API endpoint to process user queries using the RAG system.\n",
        "    \"\"\"\n",
        "    user_query = request.query\n",
        "    response = rag_query(user_query)\n",
        "    return {\"response\": response}\n",
        "\n",
        "# Include the code to run the FastAPI application\n",
        "if __name__ == \"__main__\":\n",
        "    # Note: In a production environment, you might want to configure host and port\n",
        "    # and potentially use a process manager like Gunicorn.\n",
        "    # For this example, we'll run directly with uvicorn.\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=3000)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [1406]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:3000 (Press CTRL+C to quit)\n",
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [1406]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ce2168d"
      },
      "source": [
        "**Reasoning**:\n",
        "The error `RuntimeError: asyncio.run() cannot be called from a running event loop` indicates that `uvicorn.run()` is being called within an environment that already has a running asyncio event loop, such as a Jupyter Notebook. To run the FastAPI application within this environment for testing or demonstration purposes without blocking the notebook execution, we can use `nest_asyncio`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6192287f"
      },
      "source": [
        "## (bonus) implement rag evaluation\n",
        "\n",
        "### Subtask:\n",
        "Develop a method to evaluate the groundedness and relevance of the generated answers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff6415d8"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a function `evaluate_rag_answer` that takes query, context, and answer as input and uses keyword overlap and semantic similarity to assess groundedness and relevance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "da7a030b"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_rag_answer(query: str, retrieved_context: str, generated_answer: str, model) -> dict:\n",
        "    \"\"\"\n",
        "    Evaluates the groundedness and relevance of a generated answer.\n",
        "\n",
        "    Args:\n",
        "        query: The original user query.\n",
        "        retrieved_context: The text retrieved from the document.\n",
        "        generated_answer: The answer generated by the language model.\n",
        "        model: The sentence transformer model for generating embeddings.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing evaluation metrics (groundedness and relevance scores).\n",
        "    \"\"\"\n",
        "    eval_results = {\n",
        "        \"groundedness_score\": 0.0,\n",
        "        \"relevance_score\": 0.0,\n",
        "        \"groundedness_judgment\": \"Unsupported\",\n",
        "        \"relevance_judgment\": \"Irrelevant\"\n",
        "    }\n",
        "\n",
        "    if not generated_answer or not retrieved_context:\n",
        "        # Cannot evaluate if there's no answer or context\n",
        "        return eval_results\n",
        "\n",
        "    # --- Groundedness Evaluation ---\n",
        "    # Method 1: Keyword Overlap (Simple approach)\n",
        "    # Check if key terms from the answer are present in the context\n",
        "    answer_words = set(re.findall(r'\\b\\w+\\b', generated_answer.lower()))\n",
        "    context_words = set(re.findall(r'\\b\\w+\\b', retrieved_context.lower()))\n",
        "    common_words = answer_words.intersection(context_words)\n",
        "    # Simple groundedness score based on word overlap ratio (can be refined)\n",
        "    if len(answer_words) > 0:\n",
        "        eval_results[\"groundedness_score\"] = len(common_words) / len(answer_words)\n",
        "\n",
        "    # Method 2: Semantic Similarity (Using sentence embeddings)\n",
        "    # Compare the similarity between the answer and the context\n",
        "    try:\n",
        "        answer_embedding = model.encode(generated_answer)\n",
        "        context_embedding = model.encode(retrieved_context)\n",
        "        # Reshape for cosine_similarity calculation if they are 1D arrays\n",
        "        if answer_embedding.ndim == 1:\n",
        "             answer_embedding = answer_embedding.reshape(1, -1)\n",
        "        if context_embedding.ndim == 1:\n",
        "             context_embedding = context_embedding.reshape(1, -1)\n",
        "\n",
        "        # Calculate cosine similarity\n",
        "        semantic_groundedness = cosine_similarity(answer_embedding, context_embedding)[0][0]\n",
        "        # Combine semantic similarity with keyword overlap (optional, adjust weighting)\n",
        "        # eval_results[\"groundedness_score\"] = (eval_results[\"groundedness_score\"] + semantic_groundedness) / 2\n",
        "        eval_results[\"semantic_groundedness_score\"] = float(semantic_groundedness)\n",
        "\n",
        "        # Judgment based on a threshold (can be refined)\n",
        "        if eval_results[\"groundedness_score\"] > 0.2 or semantic_groundedness > 0.5: # Example thresholds\n",
        "             eval_results[\"groundedness_judgment\"] = \"Supported\"\n",
        "        else:\n",
        "             eval_results[\"groundedness_judgment\"] = \"Unsupported\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during groundedness semantic similarity calculation: {e}\")\n",
        "\n",
        "\n",
        "    # --- Relevance Evaluation ---\n",
        "    # Method 1: Keyword Overlap (Simple approach)\n",
        "    # Check if key terms from the query are present in the answer\n",
        "    query_words = set(re.findall(r'\\b\\w+\\b', query.lower()))\n",
        "    answer_words_for_relevance = set(re.findall(r'\\b\\w+\\b', generated_answer.lower())) # Recalculate to be safe\n",
        "    common_query_answer_words = query_words.intersection(answer_words_for_relevance)\n",
        "    # Simple relevance score based on word overlap ratio (can be refined)\n",
        "    if len(query_words) > 0:\n",
        "        eval_results[\"relevance_score\"] = len(common_query_answer_words) / len(query_words)\n",
        "\n",
        "\n",
        "    # Method 2: Semantic Similarity (Using sentence embeddings)\n",
        "    # Compare the similarity between the query and the answer\n",
        "    try:\n",
        "        query_embedding = model.encode(query)\n",
        "        # Ensure query_embedding is 2D for cosine_similarity\n",
        "        if query_embedding.ndim == 1:\n",
        "            query_embedding = query_embedding.reshape(1, -1)\n",
        "\n",
        "        semantic_relevance = cosine_similarity(query_embedding, answer_embedding)[0][0]\n",
        "        # Combine semantic similarity with keyword overlap (optional)\n",
        "        # eval_results[\"relevance_score\"] = (eval_results[\"relevance_score\"] + semantic_relevance) / 2\n",
        "        eval_results[\"semantic_relevance_score\"] = float(semantic_relevance)\n",
        "\n",
        "        # Judgment based on a threshold (can be refined)\n",
        "        if eval_results[\"relevance_score\"] > 0.2 or semantic_relevance > 0.5: # Example thresholds\n",
        "            eval_results[\"relevance_judgment\"] = \"Relevant\"\n",
        "        else:\n",
        "            eval_results[\"relevance_judgment\"] = \"Irrelevant\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during relevance semantic similarity calculation: {e}\")\n",
        "\n",
        "\n",
        "    return eval_results\n",
        "\n",
        "# Example Usage (for testing)\n",
        "# Note: Replace with actual model and sample data\n",
        "# try:\n",
        "#     # Assuming 'model' is already loaded from previous steps\n",
        "#     sample_query_en = \"What is the main theme of the chapter?\"\n",
        "#     sample_context_en = \"The main theme of this chapter is the importance of education...\"\n",
        "#     sample_answer_en = \"The main theme discussed is the significance of learning.\"\n",
        "#\n",
        "#     sample_query_bn = \"কবিতার মূলভাব কী?\"\n",
        "#     sample_context_bn = \"কবিতাটির মূলভাব হলো প্রকৃতির রূপ বর্ণনা...\"\n",
        "#     sample_answer_bn = \"এই কবিতার মূলভাব হলো প্রকৃতির সৌন্দর্য।\"\n",
        "#\n",
        "#     print(\"\\n--- English Example ---\")\n",
        "#     eval_en = evaluate_rag_answer(sample_query_en, sample_context_en, sample_answer_en, model)\n",
        "#     print(f\"Evaluation Results (English): {eval_en}\")\n",
        "#\n",
        "#     print(\"\\n--- Bengali Example ---\")\n",
        "#     eval_bn = evaluate_rag_answer(sample_query_bn, sample_context_bn, sample_answer_bn, model)\n",
        "#     print(f\"Evaluation Results (Bengali): {eval_bn}\")\n",
        "#\n",
        "# except NameError:\n",
        "#     print(\"\\nSentenceTransformer model 'model' is not defined. Please run the vectorization step first.\")\n",
        "# except Exception as e:\n",
        "#      print(f\"\\nAn error occurred during example evaluation: {e}\")\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3844740e",
        "outputId": "80ace0a4-d80e-4893-80c3-b3637a139303"
      },
      "source": [
        "# Install ngrok\n",
        "!pip install pyngrok"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.12-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.12-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-1' coro=<Server.serve() done, defined at /usr/local/lib/python3.11/dist-packages/uvicorn/server.py:69> exception=KeyboardInterrupt()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/main.py\", line 580, in run\n",
            "    server.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 67, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 30, in run\n",
            "    return loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 360, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 70, in serve\n",
            "    with self.capture_signals():\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 144, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 331, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6866c34e",
        "outputId": "4e280f57-229f-4bb1-c63f-514f212c9c2b"
      },
      "source": [
        "# Authenticate ngrok (You'll need an ngrok auth token from your ngrok account)\n",
        "# Replace 'YOUR_AUTHTOKEN' with your actual ngrok auth token\n",
        "from pyngrok import ngrok\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get your authtoken from https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "# Add your token to Colab secrets with the name 'NGROK_AUTH_TOKEN'\n",
        "NGROK_AUTH_TOKEN = userdata.get(\"NGROK_AUTH_TOKEN\")\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# Start ngrok tunnel for port 8000\n",
        "# The ngrok process will run in the background\n",
        "ngrok_tunnel = ngrok.connect(3000)\n",
        "print(f\"Public URL: {ngrok_tunnel.public_url}\")"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: https://883cd7a2e6d9.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Mostafa-Annur/AI_Assessment.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehoBZ_o6bNEW",
        "outputId": "07b27624-8e71-4e1a-a617-a277f194b38e"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AI_Assessment'...\n",
            "remote: Enumerating objects: 16, done.\u001b[K\n",
            "remote: Counting objects:   6% (1/16)\u001b[K\rremote: Counting objects:  12% (2/16)\u001b[K\rremote: Counting objects:  18% (3/16)\u001b[K\rremote: Counting objects:  25% (4/16)\u001b[K\rremote: Counting objects:  31% (5/16)\u001b[K\rremote: Counting objects:  37% (6/16)\u001b[K\rremote: Counting objects:  43% (7/16)\u001b[K\rremote: Counting objects:  50% (8/16)\u001b[K\rremote: Counting objects:  56% (9/16)\u001b[K\rremote: Counting objects:  62% (10/16)\u001b[K\rremote: Counting objects:  68% (11/16)\u001b[K\rremote: Counting objects:  75% (12/16)\u001b[K\rremote: Counting objects:  81% (13/16)\u001b[K\rremote: Counting objects:  87% (14/16)\u001b[K\rremote: Counting objects:  93% (15/16)\u001b[K\rremote: Counting objects: 100% (16/16)\u001b[K\rremote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects:   8% (1/12)\u001b[K\rremote: Compressing objects:  16% (2/12)\u001b[K\rremote: Compressing objects:  25% (3/12)\u001b[K\rremote: Compressing objects:  33% (4/12)\u001b[K\rremote: Compressing objects:  41% (5/12)\u001b[K\rremote: Compressing objects:  50% (6/12)\u001b[K\rremote: Compressing objects:  58% (7/12)\u001b[K\rremote: Compressing objects:  66% (8/12)\u001b[K\rremote: Compressing objects:  75% (9/12)\u001b[K\rremote: Compressing objects:  83% (10/12)\u001b[K\rremote: Compressing objects:  91% (11/12)\u001b[K\rremote: Compressing objects: 100% (12/12)\u001b[K\rremote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "Receiving objects:   6% (1/16)\rReceiving objects:  12% (2/16)\rReceiving objects:  18% (3/16)\rReceiving objects:  25% (4/16)\rReceiving objects:  31% (5/16)\rReceiving objects:  37% (6/16)\rReceiving objects:  43% (7/16)\rReceiving objects:  50% (8/16)\rReceiving objects:  56% (9/16)\rremote: Total 16 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects:  62% (10/16)\rReceiving objects:  68% (11/16)\rReceiving objects:  75% (12/16)\rReceiving objects:  81% (13/16)\rReceiving objects:  87% (14/16)\rReceiving objects:  93% (15/16)\rReceiving objects: 100% (16/16)\rReceiving objects: 100% (16/16), 26.89 KiB | 13.45 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3bd4df1",
        "outputId": "be636688-0de0-4bcb-a491-48c41cc3f08a"
      },
      "source": [
        "# Install git (if not already installed, Colab usually has it)\n",
        "# !apt-get update && apt-get install -y git\n",
        "\n",
        "# Configure Git (replace with your name and email)\n",
        "!git config --global user.name \"Mostafa-Annur\"\n",
        "!git config --global user.email \"mostafaannur@gmail.com\"\n",
        "\n",
        "# Clone your empty GitHub repository (replace with your repo URL)\n",
        "# You might need to use a Personal Access Token for authentication if prompted\n",
        "!git clone https://github.com/Mostafa-Annur/AI_Assessment.git\n",
        "\n",
        "# Navigate into your repository directory\n",
        "import os\n",
        "os.chdir('AI_Assessment')\n",
        "\n",
        "# Copy your notebook file into the repository directory\n",
        "# Replace 'your_notebook_name.ipynb' with the actual filename of your notebook\n",
        "!cp /content/AI_Assessment.ipynb .\n",
        "\n",
        "# Add the notebook file\n",
        "# Replace 'your_notebook_name.ipynb' with the actual filename of your notebook\n",
        "!git add AI_Assessment.ipynb\n",
        "\n",
        "# Commit the changes\n",
        "!git commit -m \"Add RAG notebook\"\n",
        "\n",
        "# Push to the remote repository\n",
        "!git push origin main # or master, depending on your branch name\n",
        "\n",
        "# Note: Uncomment and execute the commands above one by one after replacing placeholders."
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'AI_Assessment' already exists and is not an empty directory.\n",
            "[main d0ca33f] Add RAG notebook\n",
            " 1 file changed, 998 insertions(+)\n",
            " create mode 100644 AI_Assessment.ipynb\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0d7fb5a"
      },
      "source": [
        "## RAG Project Setup Guide\n",
        "\n",
        "This guide outlines the steps to set up and run the Retrieval Augmented Generation (RAG) system.\n",
        "\n",
        "### 1. Load and preprocess the PDF document\n",
        "\n",
        "-   Execute the code cell with the title \"**Load and preprocess the pdf document**\" (cell ID `bfa946c6`). This will extract text from the PDF, clean it, and print basic information.\n",
        "\n",
        "### 2. Chunk the document\n",
        "\n",
        "-   Execute the code cell with the title \"**Chunk the document**\" (cell ID `8457b411`). This will split the cleaned text into smaller chunks with specified size and overlap.\n",
        "\n",
        "### 3. Vectorize the chunks\n",
        "\n",
        "-   Execute the code cell with the title \"**Vectorize the chunks**\" (cell ID `ee6bcb46`). This will load a Sentence Transformer model and generate embeddings for each chunk.\n",
        "\n",
        "### 4. Create a Vector Database\n",
        "\n",
        "-   Execute the code cell with the title \"**Create a Vector Database**\" (cell ID `GVNkMsDFS0iI`). This will create a FAISS vector store using the generated embeddings.\n",
        "\n",
        "### 5. Implement the RAG System\n",
        "\n",
        "-   Execute the code cell with the title \"**Implement the rag system**\" (cell ID `c107b370`). This defines the `rag_query` function using a pre-trained language model (Flan-T5 small). Note that the model loading might require specific libraries or handling depending on your environment and model availability.\n",
        "-   Execute the subsequent code cell (cell ID `XQ48m4K4TCu4`) which attempts to load the `text2text-generation` pipeline for Flan-T5.\n",
        "\n",
        "### 6. (Bonus) Implement a Conversation API\n",
        "\n",
        "-   Execute the code cell defining the FastAPI app and QueryRequest model (cell ID `f2574ac0`).\n",
        "-   Execute the code cell defining the `/query` endpoint (cell ID `3dc3b459`).\n",
        "-   Execute the code cell applying `nest_asyncio` (cell ID `ewoDQEhUTPQj`) to allow the API to run in a notebook environment.\n",
        "-   **To run the API:** You will need to execute the `uvicorn.run(app, host=\"0.0.0.0\", port=3000)` command. This is typically done in a separate cell or script, but in this notebook context, you would uncomment and run the `uvicorn.run` line within the cell where it's defined, or use `nest_asyncio` as applied in cell `ewoDQEhUTPQj` and then run cell `3dc3b459`.\n",
        "\n",
        "### 7. (Bonus) Implement RAG Evaluation\n",
        "\n",
        "-   Execute the code cell defining the `evaluate_rag_answer` function (cell ID `da7a030b`). This provides a method to evaluate the quality of the generated answers.\n",
        "\n",
        "### 8. (Bonus) Expose API with ngrok\n",
        "\n",
        "-   Execute the code cell to install `pyngrok` (cell ID `3844740e`).\n",
        "-   Execute the code cell to authenticate ngrok and create a tunnel (cell ID `6866c34e`). Remember to add your ngrok auth token to Colab secrets and replace the placeholder. This will provide a public URL to access your FastAPI.\n",
        "\n",
        "### 9. (Bonus) Push to GitHub\n",
        "\n",
        "-   Use the commands provided in the relevant code cells (e.g., cell ID `b3bd4df1`) to clone your repository, copy the notebook, commit changes, and push to GitHub. Remember to replace placeholder values and uncomment lines as needed.\n",
        "\n",
        "---\n",
        "\n",
        "After completing these steps, your RAG system should be set up, and you can test it by calling the `rag_query` function or interacting with the FastAPI endpoint if you set it up."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dda273b6"
      },
      "source": [
        "## Sample Queries and Outputs\n",
        "\n",
        "Here are examples of how to use the `rag_query` function with sample queries in both English and Bengali, based on the provided document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e279df8",
        "outputId": "ff857774-1466-4f33-9340-c6dd6e5360f4"
      },
      "source": [
        "# Example Usage of rag_query\n",
        "# Assuming the previous cells (loading, chunking, vectorizing, vector store, RAG implementation) have been run successfully.\n",
        "\n",
        "english_query = \"What is the main topic of the document?\"\n",
        "bangla_query = \"অনুপম কী কাজ করে জীবিকা নির্বাহ করতেন?\"\n",
        "\n",
        "print(f\"English Query: {english_query}\")\n",
        "english_response = rag_query(english_query)\n",
        "print(f\"Response: {english_response}\")\n",
        "\n",
        "print(f\"\\nBengali Query: {bangla_query}\")\n",
        "bangla_response = rag_query(bangla_query)\n",
        "print(f\"Response: {bangla_response}\")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English Query: What is the main topic of the document?\n",
            "Response:                                                                           \n",
            "\n",
            "Bengali Query: অনুপম কী কাজ করে জীবিকা নির্বাহ করতেন?\n",
            "Response:                                                                           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d30d89c1"
      },
      "source": [
        "## API Documentation\n",
        "\n",
        "If you have successfully run the FastAPI application (by executing the cell with `uvicorn.run` after applying `nest_asyncio`), you can access the automatic API documentation provided by FastAPI.\n",
        "\n",
        "-   **Swagger UI:** Go to the ngrok public URL (obtained from the ngrok cell) followed by `/docs`. For example: `YOUR_NGROK_URL/docs`\n",
        "-   **ReDoc:** Go to the ngrok public URL followed by `/redoc`. For example: `YOUR_NGROK_URL/redoc`\n",
        "\n",
        "These pages will provide an interactive interface to see the available endpoints (`/query`), their expected request body, and try them out directly in your browser."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99fe054a"
      },
      "source": [
        "## Evaluation Matrix Example\n",
        "\n",
        "Here's how you can use the `evaluate_rag_answer` function to evaluate the quality of the generated answers based on the query and the retrieved context."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ebcea7a",
        "outputId": "c9bc8cb3-811d-40d3-9b10-67fe2039da21"
      },
      "source": [
        "# Example Usage of evaluate_rag_answer\n",
        "# Assuming the previous cells (loading, chunking, vectorizing, vector store, RAG implementation, evaluation function) have been run successfully.\n",
        "\n",
        "# To evaluate an answer, you need the original query, the context retrieved for that query, and the generated answer.\n",
        "# We can simulate this using a query and then retrieving context and generating an answer using our RAG function.\n",
        "\n",
        "# Example Query\n",
        "eval_query = \"দীপু কে ছিলেন?\"\n",
        "\n",
        "# Get the generated answer using the RAG function\n",
        "eval_answer = rag_query(eval_query)\n",
        "print(f\"Query for Evaluation: {eval_query}\")\n",
        "print(f\"Generated Answer: {eval_answer}\")\n",
        "\n",
        "# To get the retrieved context for evaluation, we need to explicitly perform the similarity search again.\n",
        "# This assumes 'model' and 'vectorstore' are available from previous steps.\n",
        "try:\n",
        "    if 'model' in globals() and model is not None and 'vectorstore' in globals() and vectorstore is not None:\n",
        "        query_embedding_eval = model.encode(eval_query)\n",
        "        # Retrieve the documents used for this query\n",
        "        retrieved_docs_eval = vectorstore.similarity_search_by_vector(query_embedding_eval, k=3) # Use the same k as in rag_query\n",
        "        eval_context = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in retrieved_docs_eval])\n",
        "        print(f\"\\nRetrieved Context:\\n{eval_context}\")\n",
        "\n",
        "        # Evaluate the generated answer\n",
        "        evaluation_results = evaluate_rag_answer(eval_query, eval_context, eval_answer, model)\n",
        "        print(f\"\\nEvaluation Results: {evaluation_results}\")\n",
        "    else:\n",
        "        print(\"\\nSentenceTransformer model or vector store not initialized. Cannot perform evaluation example.\")\n",
        "\n",
        "except AttributeError:\n",
        "     print(\"\\nError: 'similarity_search_by_vector' method not found on the vectorstore object. Cannot perform evaluation example.\")\n",
        "except Exception as e:\n",
        "     print(f\"\\nAn error occurred during evaluation example: {e}\")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query for Evaluation: দীপু কে ছিলেন?\n",
            "Generated Answer: ()  ()  ()  ()  ()  ()  ()  ()  ()  ()  ()  ()  ()  ()  ()  ()  ()  ()  ()  ()  ()  ()  ()  ()  ()  ()  ()  ()  ()  () \n",
            "\n",
            "Retrieved Context:\n",
            " (গ) হর্িকিি (ঘ) মাক ি ৩৩। কষ্টিপাথির্নক যকব্কসর্িল? (ক) মামা (খ) সযাকিা (গ) র্ব্নুদাদা (ঘ) হর্িি ৩৪। 'এ ার্িং' যকাথাযথককআনাহক কি? (ক) র্ব্কলত (খ) কানপুি (গ) কজলকাতা (ঘ) আন্দামান ৩৫। 'ঠাট্টািসম্পকিটাককিা ীকর্িব্ািইোআমািনাই'- উজক্তটি (ক) র্ব্নুদাদাি (খ) অনুপকমি (গ) মামাি (ঘ) িম্ভুনাকথি ব্াব্ুি ৩৬। অনুপমকাককর্নক তীথিোত্রাশুরুককি? (ক) কলযাণীকক (খ) মাকক (গ) হর্িিকক (ঘ) র্ব্নুদাদাকক ৩৭। মা-পুকত্রিতীথিোত্রািব্াহনকীর্িল? (ক) যিলগার়্ে (খ) গরুি গার়্ে (গ) যমাটি গার়্ে (ঘ) যঘা়োি গার়্ে ৩৮। 'অন্নপূণি\n",
            "\n",
            "---\n",
            "\n",
            "র্িব্াকিিকতিাব্যজক্তকদিওপির্নভিিকিকতহ ।তাইর্ব্ক ি মকতাগুরুত্বপূণির্সিাকন্তিযক্ষকত্রওতািাপর্িব্াকিিপিন্দ-অপিকন্দিওপির্নভিিককি। উদ্দীপককিপািকভিস্পষ্ট্ব্াদীওব্যজক্তত্বব্ান।যসর্নকিির্সিান্তর্নকির্নকতপাকি।একািকণইযস যেৌতুককলাভীব্াব্ািকথািব্াইকির্গক র্ব্ক িকথাব্কলকি।যসযকাকনাদিদামব্াযব্চাককনািপণযন । যসএকিনককিীব্নসঙ্গীকিকতএকসকি, অপমানকিকতন ।'অপর্ির্চতা' গকেিঅনুপমওর্িজক্ষত, মাজিত। 21 সৃজনশী প্রশ্ন র্কন্তুস্পষ্ট্কথাব্লািমকতাসাহসতািযনই।র্নকিির্সিান্তযসর্নকির্নকতপাকিনা।যসকিার্দক গহনা োচাইযেককনপকক্ষিঅপমানতাঅ\n",
            "\n",
            "---\n",
            "\n",
            "কতপািকব্। ✓তৎকাকলসমাকিভদ্রকলাককিস্বভাব্বব্র্িষ্ট্যসম্পককিজ্ঞাানলাভকিকব্। ✓নািীযকামলঠিক, র্কন্তুদুব্িলন - কলযাণীিিীব্নচর্িতদ্বািাপ্রর্তজিতএইসতযঅনুধাব্নকিকত পািকব্। ✓মানুষআিার্নক যব্ঁকচথাকক- অনুপকমিদৃষ্ট্াকন্তমানব্িীব্কনিএইর্চিন্তনসতযদিনসম্পককি জ্ঞাানলাভকিকব্। র্িখনফল 2 শব্দার্ব ও টীকা েূ শব্দ শলব্দি অর্ব ও িযাখ্যা এ িীব্নটা না দদকঘিযি র্হসাকব্ব্ক়ো, না গুকণি র্হসাকব্ গকেি কথক চর্িত্র অনুপকমি আত্মসমাকলাচনা। পর্িমাণ ও গুণ উভ র্দক র্দক ই যে তাি িীব্নটি র্নতান্তই তুে যস কথাই এখাকন ব্যক্ত হক কি। ফকলি\n",
            "\n",
            "Evaluation Results: {'groundedness_score': 0.0, 'relevance_score': 0.0, 'groundedness_judgment': 'Supported', 'relevance_judgment': 'Irrelevant', 'semantic_groundedness_score': 0.538219153881073, 'semantic_relevance_score': 0.2059863805770874}\n"
          ]
        }
      ]
    }
  ]
}